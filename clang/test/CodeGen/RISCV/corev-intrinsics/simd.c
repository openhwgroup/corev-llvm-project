// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv32 -target-feature +xcvsimd -emit-llvm %s -o - \
// RUN:     | FileCheck %s

#include <stdint.h>

// CHECK-LABEL: @test_add_h_div1(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.add.h(i32 [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_h_div1(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_add_h(a, b, 0);
}

// CHECK-LABEL: @test_add_h_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.add.h(i32 [[TMP0]], i32 [[TMP1]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_h_div2(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_add_h(a, b, 1);
}

// CHECK-LABEL: @test_add_h_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.add.h(i32 [[TMP0]], i32 [[TMP1]], i32 2)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_h_div4(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_add_h(a, b, 2);
}

// CHECK-LABEL: @test_add_h_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.add.h(i32 [[TMP0]], i32 [[TMP1]], i32 3)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_h_div8(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_add_h(a, b, 3);
}

// CHECK-LABEL: @test_add_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.add.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_add_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_add_b(a, b);
}

// CHECK-LABEL: @test_add_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.add.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_add_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_add_sc_h(a, b);
}

// CHECK-LABEL: @test_add_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.add.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_add_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_add_sc_h(a, 5);
}

// CHECK-LABEL: @test_add_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.add.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_add_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_add_sc_h(a, -32);
}

// CHECK-LABEL: @test_add_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.add.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_add_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_add_sc_b(a, b);
}

// CHECK-LABEL: @test_add_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.add.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_add_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_add_sc_b(a, 5);
}

// CHECK-LABEL: @test_add_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.add.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_add_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_add_sc_b(a, -32);
}

// CHECK-LABEL: @test_sub_h_div1(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sub.h(i32 [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_h_div1(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_sub_h(a, b, 0);
}

// CHECK-LABEL: @test_sub_h_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sub.h(i32 [[TMP0]], i32 [[TMP1]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_h_div2(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_sub_h(a, b, 1);
}

// CHECK-LABEL: @test_sub_h_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sub.h(i32 [[TMP0]], i32 [[TMP1]], i32 2)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_h_div4(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_sub_h(a, b, 2);
}

// CHECK-LABEL: @test_sub_h_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sub.h(i32 [[TMP0]], i32 [[TMP1]], i32 3)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_h_div8(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_sub_h(a, b, 3);
}

// CHECK-LABEL: @test_sub_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sub.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sub_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_sub_b(a, b);
}

// CHECK-LABEL: @test_sub_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sub.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sub_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_sub_sc_h(a, b);
}

// CHECK-LABEL: @test_sub_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sub.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sub_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_sub_sc_h(a, 5);
}

// CHECK-LABEL: @test_sub_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sub.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sub_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_sub_sc_h(a, -32);
}

// CHECK-LABEL: @test_sub_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sub.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sub_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_sub_sc_b(a, b);
}

// CHECK-LABEL: @test_sub_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sub.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sub_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_sub_sc_b(a, 5);
}

// CHECK-LABEL: @test_sub_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sub.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sub_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_sub_sc_b(a, -32);
}

// CHECK-LABEL: @test_avg_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.avg.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avg_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_avg_h(a, b);
}

// CHECK-LABEL: @test_avg_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.avg.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avg_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_avg_b(a, b);
}

// CHECK-LABEL: @test_avg_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.avg.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_avg_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_avg_sc_h(a, b);
}

// CHECK-LABEL: @test_avg_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.avg.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avg_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_avg_sc_h(a, 5);
}

// CHECK-LABEL: @test_avg_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.avg.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avg_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_avg_sc_h(a, -32);
}

// CHECK-LABEL: @test_avg_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.avg.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_avg_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_avg_sc_b(a, b);
}

// CHECK-LABEL: @test_avg_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.avg.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avg_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_avg_sc_b(a, 5);
}

// CHECK-LABEL: @test_avg_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.avg.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avg_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_avg_sc_b(a, -32);
}

// CHECK-LABEL: @test_avgu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.avgu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avgu_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_avgu_h(a, b);
}

// CHECK-LABEL: @test_avgu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.avgu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_avgu_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_avgu_b(a, b);
}

// CHECK-LABEL: @test_avgu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.avgu.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_avgu_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_avgu_sc_h(a, b);
}

// CHECK-LABEL: @test_avgu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.avgu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avgu_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_avgu_sc_h(a, 5);
}

// CHECK-LABEL: @test_avgu_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.avgu.sc.h(i32 [[TMP0]], i32 65504)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avgu_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_avgu_sc_h(a, -32);
}

// CHECK-LABEL: @test_avgu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.avgu.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_avgu_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_avgu_sc_b(a, b);
}

// CHECK-LABEL: @test_avgu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.avgu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avgu_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_avgu_sc_b(a, 5);
}

// CHECK-LABEL: @test_avgu_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.avgu.sc.b(i32 [[TMP0]], i32 224)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_avgu_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_avgu_sc_b(a, -32);
}

// CHECK-LABEL: @test_min_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.min.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_min_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_min_h(a, b);
}

// CHECK-LABEL: @test_min_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.min.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_min_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_min_b(a, b);
}

// CHECK-LABEL: @test_min_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.min.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_min_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_min_sc_h(a, b);
}

// CHECK-LABEL: @test_min_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.min.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_min_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_min_sc_h(a, 5);
}

// CHECK-LABEL: @test_min_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.min.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_min_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_min_sc_h(a, -32);
}

// CHECK-LABEL: @test_min_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.min.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_min_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_min_sc_b(a, b);
}

// CHECK-LABEL: @test_min_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.min.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_min_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_min_sc_b(a, 5);
}

// CHECK-LABEL: @test_min_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.min.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_min_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_min_sc_b(a, -32);
}

// CHECK-LABEL: @test_minu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.minu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_minu_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_minu_h(a, b);
}

// CHECK-LABEL: @test_minu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.minu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_minu_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_minu_b(a, b);
}

// CHECK-LABEL: @test_minu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.minu.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_minu_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_minu_sc_h(a, b);
}

// CHECK-LABEL: @test_minu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.minu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_minu_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_minu_sc_h(a, 5);
}

// CHECK-LABEL: @test_minu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.minu.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_minu_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_minu_sc_b(a, b);
}

// CHECK-LABEL: @test_minu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.minu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_minu_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_minu_sc_b(a, 5);
}

// CHECK-LABEL: @test_max_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.max.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_max_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_max_h(a, b);
}

// CHECK-LABEL: @test_max_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.max.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_max_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_max_b(a, b);
}

// CHECK-LABEL: @test_max_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.max.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_max_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_max_sc_h(a, b);
}

// CHECK-LABEL: @test_max_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.max.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_max_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_max_sc_h(a, 5);
}

// CHECK-LABEL: @test_max_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.max.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_max_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_max_sc_h(a, -32);
}

// CHECK-LABEL: @test_max_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.max.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_max_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_max_sc_b(a, b);
}

// CHECK-LABEL: @test_max_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.max.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_max_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_max_sc_b(a, 5);
}

// CHECK-LABEL: @test_max_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.max.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_max_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_max_sc_b(a, -32);
}

// CHECK-LABEL: @test_maxu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.maxu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_maxu_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_maxu_h(a, b);
}

// CHECK-LABEL: @test_maxu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.maxu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_maxu_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_maxu_b(a, b);
}

// CHECK-LABEL: @test_maxu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.maxu.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_maxu_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_maxu_sc_h(a, b);
}

// CHECK-LABEL: @test_maxu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.maxu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_maxu_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_maxu_sc_h(a, 5);
}

// CHECK-LABEL: @test_maxu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.maxu.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_maxu_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_maxu_sc_b(a, b);
}

// CHECK-LABEL: @test_maxu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.maxu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_maxu_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_maxu_sc_b(a, 5);
}

// CHECK-LABEL: @test_srl_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.srl.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_srl_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_srl_h(a, b);
}

// CHECK-LABEL: @test_srl_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.srl.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_srl_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_srl_b(a, b);
}

// CHECK-LABEL: @test_srl_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = trunc i16 [[TMP1]] to i8
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[CONV]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.srl.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_srl_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_srl_sc_h(a, b);
}

// CHECK-LABEL: @test_srl_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.srl.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_srl_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_srl_sc_h(a, 5);
}

// CHECK-LABEL: @test_srl_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.srl.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_srl_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_srl_sc_b(a, b);
}

// CHECK-LABEL: @test_srl_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.srl.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_srl_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_srl_sc_b(a, 5);
}

// CHECK-LABEL: @test_sra_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sra.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sra_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_sra_h(a, b);
}

// CHECK-LABEL: @test_sra_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sra.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sra_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_sra_b(a, b);
}

// CHECK-LABEL: @test_sra_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = trunc i16 [[TMP1]] to i8
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[CONV]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sra.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sra_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_sra_sc_h(a, b);
}

// CHECK-LABEL: @test_sra_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sra.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sra_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_sra_sc_h(a, 5);
}

// CHECK-LABEL: @test_sra_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sra.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sra_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_sra_sc_b(a, b);
}

// CHECK-LABEL: @test_sra_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sra.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sra_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_sra_sc_b(a, 5);
}

// CHECK-LABEL: @test_sll_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sll.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sll_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_sll_h(a, b);
}

// CHECK-LABEL: @test_sll_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sll.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sll_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_sll_b(a, b);
}

// CHECK-LABEL: @test_sll_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = trunc i16 [[TMP1]] to i8
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[CONV]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sll.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sll_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_sll_sc_h(a, b);
}

// CHECK-LABEL: @test_sll_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sll.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sll_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_sll_sc_h(a, 5);
}

// CHECK-LABEL: @test_sll_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sll.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sll_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_sll_sc_b(a, b);
}

// CHECK-LABEL: @test_sll_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sll.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_sll_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_sll_sc_b(a, 5);
}

// CHECK-LABEL: @test_or_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.or.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_or_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_or_h(a, b);
}

// CHECK-LABEL: @test_or_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.or.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_or_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_or_b(a, b);
}

// CHECK-LABEL: @test_or_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.or.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_or_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_or_sc_h(a, b);
}

// CHECK-LABEL: @test_or_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.or.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_or_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_or_sc_h(a, 5);
}

// CHECK-LABEL: @test_or_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.or.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_or_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_or_sc_h(a, -32);
}

// CHECK-LABEL: @test_or_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.or.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_or_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_or_sc_b(a, b);
}

// CHECK-LABEL: @test_or_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.or.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_or_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_or_sc_b(a, 5);
}

// CHECK-LABEL: @test_or_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.or.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_or_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_or_sc_b(a, -32);
}

// CHECK-LABEL: @test_xor_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.xor.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_xor_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_xor_h(a, b);
}

// CHECK-LABEL: @test_xor_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.xor.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_xor_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_xor_b(a, b);
}

// CHECK-LABEL: @test_xor_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.xor.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_xor_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_xor_sc_h(a, b);
}

// CHECK-LABEL: @test_xor_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.xor.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_xor_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_xor_sc_h(a, 5);
}

// CHECK-LABEL: @test_xor_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.xor.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_xor_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_xor_sc_h(a, -32);
}

// CHECK-LABEL: @test_xor_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.xor.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_xor_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_xor_sc_b(a, b);
}

// CHECK-LABEL: @test_xor_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.xor.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_xor_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_xor_sc_b(a, 5);
}

// CHECK-LABEL: @test_xor_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.xor.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_xor_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_xor_sc_b(a, -32);
}

// CHECK-LABEL: @test_and_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.and.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_and_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_and_h(a, b);
}

// CHECK-LABEL: @test_and_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.and.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_and_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_and_b(a, b);
}

// CHECK-LABEL: @test_and_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.and.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_and_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_and_sc_h(a, b);
}

// CHECK-LABEL: @test_and_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.and.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_and_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_and_sc_h(a, 5);
}

// CHECK-LABEL: @test_and_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.and.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_and_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_and_sc_h(a, -32);
}

// CHECK-LABEL: @test_and_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.and.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_and_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_and_sc_b(a, b);
}

// CHECK-LABEL: @test_and_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.and.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_and_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_and_sc_b(a, 5);
}

// CHECK-LABEL: @test_and_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.and.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_and_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_and_sc_b(a, -32);
}

// CHECK-LABEL: @test_abs_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.abs.h(i32 [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_abs_h(uint32_t a) {
	return __builtin_riscv_cv_simd_abs_h(a);
}

// CHECK-LABEL: @test_abs_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.abs.b(i32 [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_abs_b(uint32_t a) {
	return __builtin_riscv_cv_simd_abs_b(a);
}

// CHECK-LABEL: @test_neg_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sub.h(i32 0, i32 [[TMP0]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_neg_h(uint32_t a) {
	return __builtin_riscv_cv_simd_neg_h(a);
}

// CHECK-LABEL: @test_neg_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.sub.b(i32 0, i32 [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_neg_b(uint32_t a) {
	return __builtin_riscv_cv_simd_neg_b(a);
}

// CHECK-LABEL: @test_dotup_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.dotup.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotup_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_dotup_h(a, b);
}

// CHECK-LABEL: @test_dotup_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.dotup.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotup_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_dotup_b(a, b);
}

// CHECK-LABEL: @test_dotup_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.dotup.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_dotup_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_dotup_sc_h(a, b);
}

// CHECK-LABEL: @test_dotup_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotup.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotup_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_dotup_sc_h(a, 5);
}

// CHECK-LABEL: @test_dotup_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.dotup.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_dotup_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_dotup_sc_b(a, b);
}

// CHECK-LABEL: @test_dotup_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotup.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotup_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_dotup_sc_b(a, 5);
}

// CHECK-LABEL: @test_dotusp_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.dotusp.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotusp_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_dotusp_h(a, b);
}

// CHECK-LABEL: @test_dotusp_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.dotusp.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotusp_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_dotusp_b(a, b);
}

// CHECK-LABEL: @test_dotusp_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.dotusp.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_dotusp_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_dotusp_sc_h(a, b);
}

// CHECK-LABEL: @test_dotusp_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotusp.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotusp_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_dotusp_sc_h(a, 5);
}

// CHECK-LABEL: @test_dotusp_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotusp.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotusp_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_dotusp_sc_h(a, -32);
}

// CHECK-LABEL: @test_dotusp_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.dotusp.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_dotusp_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_dotusp_sc_b(a, b);
}

// CHECK-LABEL: @test_dotusp_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotusp.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotusp_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_dotusp_sc_b(a, 5);
}

// CHECK-LABEL: @test_dotusp_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotusp.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotusp_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_dotusp_sc_b(a, -32);
}

// CHECK-LABEL: @test_dotsp_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.dotsp.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotsp_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_dotsp_h(a, b);
}

// CHECK-LABEL: @test_dotsp_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.dotsp.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_dotsp_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_dotsp_b(a, b);
}

// CHECK-LABEL: @test_dotsp_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.dotsp.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_dotsp_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_dotsp_sc_h(a, b);
}

// CHECK-LABEL: @test_dotsp_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotsp.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotsp_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_dotsp_sc_h(a, 5);
}

// CHECK-LABEL: @test_dotsp_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotsp.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotsp_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_dotsp_sc_h(a, -32);
}

// CHECK-LABEL: @test_dotsp_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.dotsp.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_dotsp_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_dotsp_sc_b(a, b);
}

// CHECK-LABEL: @test_dotsp_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotsp.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotsp_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_dotsp_sc_b(a, 5);
}

// CHECK-LABEL: @test_dotsp_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.dotsp.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_dotsp_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_dotsp_sc_b(a, -32);
}

// CHECK-LABEL: @test_sdotup_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sdotup.h(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sdotup_h(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotup_h(a, b, c);
}

// CHECK-LABEL: @test_sdotup_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sdotup.b(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sdotup_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotup_b(a, b, c);
}

// CHECK-LABEL: @test_sdotup_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP1]] to i16
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = zext i16 [[CONV]] to i32
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.riscv.cv.simd.sdotup.sc.h(i32 [[TMP0]], i32 [[TMP3]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP4]]
//
uint32_t test_sdotup_sc_h(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotup_sc_h(a, b, c);
}

// CHECK-LABEL: @test_sdotup_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotup.sc.h(i32 [[TMP0]], i32 5, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotup_sci_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_sdotup_sc_h(a, 5, b);
}

// CHECK-LABEL: @test_sdotup_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP1]] to i8
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = zext i8 [[CONV]] to i32
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.riscv.cv.simd.sdotup.sc.b(i32 [[TMP0]], i32 [[TMP3]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP4]]
//
uint32_t test_sdotup_sc_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotup_sc_b(a, b, c);
}

// CHECK-LABEL: @test_sdotup_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotup.sc.b(i32 [[TMP0]], i32 5, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotup_sci_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_sdotup_sc_b(a, 5, b);
}

// CHECK-LABEL: @test_sdotusp_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sdotusp.h(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sdotusp_h(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotusp_h(a, b, c);
}

// CHECK-LABEL: @test_sdotusp_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sdotusp.b(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sdotusp_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotusp_b(a, b, c);
}

// CHECK-LABEL: @test_sdotusp_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP1]] to i16
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = sext i16 [[CONV]] to i32
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.riscv.cv.simd.sdotusp.sc.h(i32 [[TMP0]], i32 [[TMP3]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP4]]
//
uint32_t test_sdotusp_sc_h(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotusp_sc_h(a, b, c);
}

// CHECK-LABEL: @test_sdotusp_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotusp.sc.h(i32 [[TMP0]], i32 5, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotusp_sci_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_sdotusp_sc_h(a, 5, b);
}

// CHECK-LABEL: @test_sdotusp_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotusp.sc.h(i32 [[TMP0]], i32 -32, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotusp_sci_h_negative(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_sdotusp_sc_h(a, -32, b);
}

// CHECK-LABEL: @test_sdotusp_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP1]] to i8
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = sext i8 [[CONV]] to i32
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.riscv.cv.simd.sdotusp.sc.b(i32 [[TMP0]], i32 [[TMP3]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP4]]
//
uint32_t test_sdotusp_sc_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotusp_sc_b(a, b, c);
}

// CHECK-LABEL: @test_sdotusp_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotusp.sc.b(i32 [[TMP0]], i32 5, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotusp_sci_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_sdotusp_sc_b(a, 5, b);
}

// CHECK-LABEL: @test_sdotusp_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotusp.sc.b(i32 [[TMP0]], i32 -32, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotusp_sci_b_negative(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_sdotusp_sc_b(a, -32, b);
}

// CHECK-LABEL: @test_sdotsp_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sdotsp.h(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sdotsp_h(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotsp_h(a, b, c);
}

// CHECK-LABEL: @test_sdotsp_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.sdotsp.b(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_sdotsp_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotsp_b(a, b, c);
}

// CHECK-LABEL: @test_sdotsp_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP1]] to i16
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = sext i16 [[CONV]] to i32
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.riscv.cv.simd.sdotsp.sc.h(i32 [[TMP0]], i32 [[TMP3]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP4]]
//
uint32_t test_sdotsp_sc_h(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotsp_sc_h(a, b, c);
}

// CHECK-LABEL: @test_sdotsp_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotsp.sc.h(i32 [[TMP0]], i32 5, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotsp_sci_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_sdotsp_sc_h(a, 5, b);
}

// CHECK-LABEL: @test_sdotsp_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotsp.sc.h(i32 [[TMP0]], i32 -32, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotsp_sci_h_negative(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_sdotsp_sc_h(a, -32, b);
}

// CHECK-LABEL: @test_sdotsp_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP1]] to i8
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = sext i8 [[CONV]] to i32
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.riscv.cv.simd.sdotsp.sc.b(i32 [[TMP0]], i32 [[TMP3]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP4]]
//
uint32_t test_sdotsp_sc_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_sdotsp_sc_b(a, b, c);
}

// CHECK-LABEL: @test_sdotsp_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotsp.sc.b(i32 [[TMP0]], i32 5, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotsp_sci_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_sdotsp_sc_b(a, 5, b);
}

// CHECK-LABEL: @test_sdotsp_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.sdotsp.sc.b(i32 [[TMP0]], i32 -32, i32 [[CONV]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_sdotsp_sci_b_negative(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_sdotsp_sc_b(a, -32, b);
}

// CHECK-LABEL: @test_extract_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.extract.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extract_h(uint32_t a) {
	return __builtin_riscv_cv_simd_extract_h(a, 5);
}

// CHECK-LABEL: @test_extract_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.extract.h(i32 [[TMP0]], i32 224)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extract_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_extract_h(a, -32);
}

// CHECK-LABEL: @test_extract_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.extract.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extract_b(uint32_t a) {
	return __builtin_riscv_cv_simd_extract_b(a, 5);
}

// CHECK-LABEL: @test_extract_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.extract.b(i32 [[TMP0]], i32 224)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extract_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_extract_b(a, -32);
}

// CHECK-LABEL: @test_extractu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.extractu.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extractu_h(uint32_t a) {
	return __builtin_riscv_cv_simd_extractu_h(a, 5);
}

// CHECK-LABEL: @test_extractu_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.extractu.h(i32 [[TMP0]], i32 224)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extractu_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_extractu_h(a, -32);
}

// CHECK-LABEL: @test_extractu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.extractu.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extractu_b(uint32_t a) {
	return __builtin_riscv_cv_simd_extractu_b(a, 5);
}

// CHECK-LABEL: @test_extractu_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.extractu.b(i32 [[TMP0]], i32 224)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_extractu_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_extractu_b(a, -32);
}

// CHECK-LABEL: @test_shuffle_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_shuffle_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_shuffle_h(a, b);
}

// CHECK-LABEL: @test_shuffle_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_shuffle_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_shuffle_b(a, b);
}

// CHECK-LABEL: @test_shuffle_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle.sci.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffle_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_shuffle_sci_h(a, 5);
}

// CHECK-LABEL: @test_shuffle_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle.sci.h(i32 [[TMP0]], i32 224)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffle_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_shuffle_sci_h(a, -32);
}

// CHECK-LABEL: @test_shuffleI0_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle.sci.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffleI0_sci_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_shuffle_sci_b(a, 5);
}

// CHECK-LABEL: @test_shuffleI1_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle.sci.b(i32 [[TMP0]], i32 69)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffleI1_sci_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_shuffle_sci_b(a, 69);
}

// CHECK-LABEL: @test_shuffleI2_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle.sci.b(i32 [[TMP0]], i32 133)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffleI2_sci_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_shuffle_sci_b(a, 133);
}

// CHECK-LABEL: @test_shuffleI3_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle.sci.b(i32 [[TMP0]], i32 197)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_shuffleI3_sci_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_shuffle_sci_b(a, 197);
}

// CHECK-LABEL: @test_shuffle2_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle2.h(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_shuffle2_h(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_shuffle2_h(a, b, c);
}

// CHECK-LABEL: @test_shuffle2_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.shuffle2.b(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_shuffle2_b(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_shuffle2_b(a, b, c);
}

// CHECK-LABEL: @test_packhi_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.packhi.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_packhi_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_packhi_h(a, b);
}

// CHECK-LABEL: @test_packlo_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.packlo.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_packlo_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_packlo_h(a, b);
}

// CHECK-LABEL: @test_cmpeq_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpeq.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpeq_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpeq_h(a, b);
}

// CHECK-LABEL: @test_cmpeq_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpeq.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpeq_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpeq_b(a, b);
}

// CHECK-LABEL: @test_cmpeq_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpeq.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpeq_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmpeq_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpeq_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpeq.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpeq_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpeq_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpeq_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpeq.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpeq_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpeq_sc_h(a, -32);
}

// CHECK-LABEL: @test_cmpeq_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpeq.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpeq_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmpeq_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpeq_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpeq.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpeq_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpeq_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpeq_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpeq.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpeq_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpeq_sc_b(a, -32);
}

// CHECK-LABEL: @test_cmpne_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpne.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpne_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpne_h(a, b);
}

// CHECK-LABEL: @test_cmpne_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpne.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpne_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpne_b(a, b);
}

// CHECK-LABEL: @test_cmpne_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpne.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpne_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmpne_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpne_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpne.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpne_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpne_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpne_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpne.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpne_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpne_sc_h(a, -32);
}

// CHECK-LABEL: @test_cmpne_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpne.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpne_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmpne_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpne_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpne.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpne_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpne_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpne_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpne.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpne_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpne_sc_b(a, -32);
}

// CHECK-LABEL: @test_cmpgt_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgt.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgt_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpgt_h(a, b);
}

// CHECK-LABEL: @test_cmpgt_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgt.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgt_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpgt_b(a, b);
}

// CHECK-LABEL: @test_cmpgt_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgt.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpgt_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmpgt_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpgt_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgt.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgt_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpgt_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpgt_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgt.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgt_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpgt_sc_h(a, -32);
}

// CHECK-LABEL: @test_cmpgt_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgt.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpgt_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmpgt_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpgt_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgt.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgt_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpgt_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpgt_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgt.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgt_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpgt_sc_b(a, -32);
}

// CHECK-LABEL: @test_cmpge_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpge.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpge_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpge_h(a, b);
}

// CHECK-LABEL: @test_cmpge_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpge.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpge_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpge_b(a, b);
}

// CHECK-LABEL: @test_cmpge_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpge.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpge_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmpge_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpge_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpge.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpge_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpge_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpge_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpge.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpge_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpge_sc_h(a, -32);
}

// CHECK-LABEL: @test_cmpge_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpge.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpge_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmpge_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpge_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpge.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpge_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpge_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpge_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpge.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpge_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpge_sc_b(a, -32);
}

// CHECK-LABEL: @test_cmplt_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmplt.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmplt_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmplt_h(a, b);
}

// CHECK-LABEL: @test_cmplt_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmplt.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmplt_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmplt_b(a, b);
}

// CHECK-LABEL: @test_cmplt_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmplt.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmplt_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmplt_sc_h(a, b);
}

// CHECK-LABEL: @test_cmplt_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmplt.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmplt_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmplt_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmplt_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmplt.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmplt_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmplt_sc_h(a, -32);
}

// CHECK-LABEL: @test_cmplt_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmplt.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmplt_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmplt_sc_b(a, b);
}

// CHECK-LABEL: @test_cmplt_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmplt.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmplt_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmplt_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmplt_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmplt.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmplt_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmplt_sc_b(a, -32);
}

// CHECK-LABEL: @test_cmple_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmple.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmple_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmple_h(a, b);
}

// CHECK-LABEL: @test_cmple_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmple.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmple_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmple_b(a, b);
}

// CHECK-LABEL: @test_cmple_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmple.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmple_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmple_sc_h(a, b);
}

// CHECK-LABEL: @test_cmple_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmple.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmple_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmple_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmple_sci_h_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmple.sc.h(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmple_sci_h_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmple_sc_h(a, -32);
}

// CHECK-LABEL: @test_cmple_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = sext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmple.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmple_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmple_sc_b(a, b);
}

// CHECK-LABEL: @test_cmple_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmple.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmple_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmple_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmple_sci_b_negative(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmple.sc.b(i32 [[TMP0]], i32 -32)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmple_sci_b_negative(uint32_t a) {
	return __builtin_riscv_cv_simd_cmple_sc_b(a, -32);
}

// CHECK-LABEL: @test_cmpgtu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgtu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgtu_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpgtu_h(a, b);
}

// CHECK-LABEL: @test_cmpgtu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgtu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgtu_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpgtu_b(a, b);
}

// CHECK-LABEL: @test_cmpgtu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgtu.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpgtu_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmpgtu_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpgtu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgtu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgtu_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpgtu_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpgtu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgtu.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpgtu_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmpgtu_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpgtu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgtu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgtu_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpgtu_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpgeu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgeu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgeu_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpgeu_h(a, b);
}

// CHECK-LABEL: @test_cmpgeu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgeu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpgeu_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpgeu_b(a, b);
}

// CHECK-LABEL: @test_cmpgeu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgeu.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpgeu_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmpgeu_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpgeu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgeu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgeu_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpgeu_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpgeu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgeu.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpgeu_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmpgeu_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpgeu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpgeu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpgeu_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpgeu_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpltu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpltu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpltu_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpltu_h(a, b);
}

// CHECK-LABEL: @test_cmpltu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpltu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpltu_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpltu_b(a, b);
}

// CHECK-LABEL: @test_cmpltu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpltu.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpltu_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmpltu_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpltu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpltu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpltu_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpltu_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpltu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpltu.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpltu_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmpltu_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpltu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpltu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpltu_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpltu_sc_b(a, 5);
}

// CHECK-LABEL: @test_cmpleu_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpleu.h(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpleu_h(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpleu_h(a, b);
}

// CHECK-LABEL: @test_cmpleu_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.cmpleu.b(i32 [[TMP0]], i32 [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_cmpleu_b(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_cmpleu_b(a, b);
}

// CHECK-LABEL: @test_cmpleu_sc_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i16 [[B:%.*]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpleu.sc.h(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpleu_sc_h(uint32_t a, uint16_t b) {
	return __builtin_riscv_cv_simd_cmpleu_sc_h(a, b);
}

// CHECK-LABEL: @test_cmpleu_sci_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpleu.sc.h(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpleu_sci_h(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpleu_sc_h(a, 5);
}

// CHECK-LABEL: @test_cmpleu_sc_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i8 [[B:%.*]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cmpleu.sc.b(i32 [[TMP0]], i32 [[TMP2]])
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cmpleu_sc_b(uint32_t a, uint8_t b) {
	return __builtin_riscv_cv_simd_cmpleu_sc_b(a, b);
}

// CHECK-LABEL: @test_cmpleu_sci_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cmpleu.sc.b(i32 [[TMP0]], i32 5)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cmpleu_sci_b(uint32_t a) {
	return __builtin_riscv_cv_simd_cmpleu_sc_b(a, 5);
}

// CHECK-LABEL: @test_cplxmul_r_div1(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cplxmul.r(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cplxmul_r_div1(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_cplxmul_r(a, b, c, 0);
}

// CHECK-LABEL: @test_cplxmul_r_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cplxmul.r(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cplxmul_r_div2(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_cplxmul_r(a, b, c, 1);
}

// CHECK-LABEL: @test_cplxmul_r_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cplxmul.r(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 2)
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cplxmul_r_div4(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_cplxmul_r(a, b, c, 2);
}

// CHECK-LABEL: @test_cplxmul_r_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cplxmul.r(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 3)
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cplxmul_r_div8(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_cplxmul_r(a, b, c, 3);
}

// CHECK-LABEL: @test_cplxmul_i_div1(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cplxmul.i(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cplxmul_i_div1(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_cplxmul_i(a, b, c, 0);
}

// CHECK-LABEL: @test_cplxmul_i_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cplxmul.i(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cplxmul_i_div2(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_cplxmul_i(a, b, c, 1);
}

// CHECK-LABEL: @test_cplxmul_i_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cplxmul.i(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 2)
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cplxmul_i_div4(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_cplxmul_i(a, b, c, 2);
}

// CHECK-LABEL: @test_cplxmul_i_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.riscv.cv.simd.cplxmul.i(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 3)
// CHECK-NEXT:    ret i32 [[TMP3]]
//
uint32_t test_cplxmul_i_div8(uint32_t a, uint32_t b, uint32_t c) {
	return __builtin_riscv_cv_simd_cplxmul_i(a, b, c, 3);
}

// CHECK-LABEL: @test_cplxconj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.riscv.cv.simd.cplxconj(i32 [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP1]]
//
uint32_t test_cplxconj(uint32_t a) {
	return __builtin_riscv_cv_simd_cplxconj(a);
}

// CHECK-LABEL: @test_subrotmj_div1(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.subrotmj(i32 [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_subrotmj_div1(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_subrotmj(a, b, 0);
}

// CHECK-LABEL: @test_subrotmj_div2(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.subrotmj(i32 [[TMP0]], i32 [[TMP1]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_subrotmj_div2(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_subrotmj(a, b, 1);
}

// CHECK-LABEL: @test_subrotmj_div4(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.subrotmj(i32 [[TMP0]], i32 [[TMP1]], i32 2)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_subrotmj_div4(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_subrotmj(a, b, 2);
}

// CHECK-LABEL: @test_subrotmj_div8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store i32 [[A:%.*]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.riscv.cv.simd.subrotmj(i32 [[TMP0]], i32 [[TMP1]], i32 3)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
uint32_t test_subrotmj_div8(uint32_t a, uint32_t b) {
	return __builtin_riscv_cv_simd_subrotmj(a, b, 3);
}

